import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import Dataset
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.font_manager import fontManager
import matplotlib as mlp

# ===== 0. è¨­å®šä¸­æ–‡å­—å‹ï¼ˆé¿å…åœ–è¡¨ä¸­æ–‡å­—äº‚ç¢¼ï¼‰ =====
fontManager.addfont("ChineseFont.ttf")  # è«‹æ”¹ç‚ºä½ ç³»çµ±ä¸Šçš„ä¸­æ–‡å­—å‹è·¯å¾‘
mlp.rc("font", family="ChineseFont")    # è¨­å®šå…¨åŸŸå­—å‹

# ===== 1. è¼‰å…¥è³‡æ–™èˆ‡è™•ç† =====
df = pd.read_csv("data/tokenized_data.csv")  # è¼‰å…¥ä¹‹å‰é è™•ç†éçš„è³‡æ–™
label2id = {label: idx for idx, label in enumerate(sorted(df['label'].unique()))}
id2label = {v: k for k, v in label2id.items()}
df["label_id"] = df["label"].map(label2id)  # å°‡æ–‡å­—æ¨™ç±¤è½‰ç‚ºæ•¸å­—

# éæ¿¾æ¨£æœ¬å¤ªå°‘çš„é¡åˆ¥ï¼ˆé¿å…å½±éŸ¿æ¨¡å‹è©•ä¼°ï¼‰
label_counts = df["label_id"].value_counts()
df_filtered = df[df["label_id"].isin(label_counts[label_counts >= 2].index)]

# åˆ‡å‡ºèˆ‡è¨“ç·´æ™‚ä¸€è‡´çš„ test set
from sklearn.model_selection import train_test_split
_, test_df = train_test_split(
    df_filtered,
    test_size=0.2,
    stratify=df_filtered["label_id"],
    random_state=42
)

# ===== 2. è¼‰å…¥ tokenizer èˆ‡è¨“ç·´å¥½çš„æ¨¡å‹ =====
model_path = "bert_models/roberta-news"  # å„²å­˜æ¨¡å‹çš„è³‡æ–™å¤¾
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# ===== 3. å°‡ test è³‡æ–™è½‰ç‚º Hugging Face Dataset ä¸¦åš tokenization =====
test_dataset = Dataset.from_pandas(test_df.rename(columns={"label_id": "labels"})[["text", "labels"]])

def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=512)

test_dataset = test_dataset.map(tokenize, batched=True)        # æ‰¹æ¬¡æ–·è©
test_dataset = test_dataset.remove_columns(["text"])           # ç§»é™¤ text æ¬„ä½
test_dataset.set_format("torch")                               # è½‰ç‚º PyTorch æ ¼å¼

# ===== 4. åŸ·è¡Œé æ¸¬ =====
from transformers import Trainer

trainer = Trainer(model=model, tokenizer=tokenizer)
pred_output = trainer.predict(test_dataset)  # é æ¸¬è¼¸å‡ºï¼šlogitsã€æ¨™ç±¤ç­‰

# å¾ logits å–å‡ºé æ¸¬é¡åˆ¥
pred_labels = np.argmax(pred_output.predictions, axis=1)
true_labels = pred_output.label_ids

# ===== 5. è¼¸å‡ºåˆ†é¡å ±å‘Š =====
# çµ„åˆå¯¦éš›èˆ‡é æ¸¬æ¨™ç±¤å‡ºç¾éçš„é¡åˆ¥
used_label_ids = sorted(set(true_labels) | set(pred_labels))
target_names = [id2label[i] for i in used_label_ids]  # å°æ‡‰çš„æ–‡å­—æ¨™ç±¤

# å°å‡ºåˆ†é¡å ±å‘Šï¼ˆprecisionã€recallã€f1-score ç­‰ï¼‰
print("\nğŸ“‹ åˆ†é¡å ±å‘Šï¼š")
print(classification_report(true_labels, pred_labels, labels=used_label_ids, target_names=target_names))

# å„²å­˜æˆ CSV æª”
report = classification_report(true_labels, pred_labels, labels=used_label_ids, target_names=target_names, output_dict=True)
report_df = pd.DataFrame(report).T
report_df.to_csv("bert_classification_report.csv", encoding="utf-8-sig")

# ===== 6. æ··æ·†çŸ©é™£è¦–è¦ºåŒ– =====
cm = confusion_matrix(true_labels, pred_labels, labels=used_label_ids)  # è¨ˆç®—æ··æ·†çŸ©é™£

# ç¹ªåœ–è¨­å®š
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=target_names,
            yticklabels=target_names)
plt.xlabel("é æ¸¬é¡åˆ¥")
plt.ylabel("å¯¦éš›é¡åˆ¥")
plt.title("åˆ†é¡æ··æ·†çŸ©é™£")
plt.tight_layout()
plt.savefig("bert_confusion_matrix.png", dpi=300)  # å„²å­˜æˆé«˜è§£æåœ–æª”
plt.show()  # é¡¯ç¤ºåœ–è¡¨
