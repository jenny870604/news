import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer
)
import evaluate
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# ======== 1. è®€å–è³‡æ–™ ===========
print("ğŸ”„ è¼‰å…¥è³‡æ–™ä¸­...")
df = pd.read_csv("data/tokenized_data.csv")  # è¼‰å…¥å·²æ–·è©å¥½çš„æ–°èè³‡æ–™ï¼Œæ¬„ä½åŒ…å« text å’Œ label

# ======== 2. Label ç·¨ç¢¼ ===========
# å°‡é¡åˆ¥æ–‡å­—è½‰æ›ç‚ºæ•´æ•¸ id
label2id = {label: idx for idx, label in enumerate(sorted(df['label'].unique()))}
id2label = {v: k for k, v in label2id.items()}
df["label_id"] = df["label"].map(label2id)  # åŠ å…¥ label_id æ¬„ä½

# ======== 3. éæ¿¾ä½æ¨£æœ¬é¡åˆ¥ ===========
# åªä¿ç•™å‡ºç¾æ¬¡æ•¸å¤§æ–¼ç­‰æ–¼ 2 çš„é¡åˆ¥ï¼Œé¿å…éå°‘è³‡æ–™å°è‡´è¨“ç·´ä¸ç©©
label_counts = df["label_id"].value_counts()
df_filtered = df[df["label_id"].isin(label_counts[label_counts >= 2].index)]

# ======== 4. åˆ‡åˆ†è³‡æ–™é›† ===========
# åˆ†æˆè¨“ç·´é›†å’Œæ¸¬è©¦é›†ï¼Œä¸¦ä¾ç…§ label_id åˆ†å±¤å–æ¨£
train_df, test_df = train_test_split(
    df_filtered,
    test_size=0.2,
    stratify=df_filtered["label_id"],
    random_state=42
)

# ======== 5. è½‰ç‚º HuggingFace Dataset ===========
# è½‰æ›æˆ Hugging Face çš„ Dataset æ ¼å¼ï¼Œä¸¦å°‡ label_id æ¬„ä½é‡æ–°å‘½åç‚º labels
train_dataset = Dataset.from_pandas(train_df.rename(columns={"label_id": "labels"})[["text", "labels"]])
test_dataset = Dataset.from_pandas(test_df.rename(columns={"label_id": "labels"})[["text", "labels"]])

# ======== 6. Tokenizer è™•ç† ===========
# æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åç¨±
# model_name = "uer/roberta-base-finetuned-chinanews-chinese"
# model_name = "hfl/chinese-roberta-wwm-ext"
model_name= "bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)  # è¼‰å…¥ tokenizer

# å®šç¾© tokenizer å‡½å¼ï¼šåŠ ä¸Š paddingã€truncation ä¸¦é™åˆ¶æœ€å¤§é•·åº¦ç‚º 512
def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=512)

# å¥—ç”¨ tokenizer åˆ°è¨“ç·´å’Œæ¸¬è©¦è³‡æ–™é›†
train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# ç§»é™¤åŸå§‹ text æ¬„ä½ï¼Œä¸¦è¨­å®šæ ¼å¼ç‚º torch tensor
train_dataset = train_dataset.remove_columns(["text"])
test_dataset = test_dataset.remove_columns(["text"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

# ======== 7. æ¨¡å‹æº–å‚™ ===========
# è¼‰å…¥é è¨“ç·´çš„ BERT æ¨¡å‹ï¼Œä¸¦è¨­å®šåˆ†é¡é¡åˆ¥æ•¸é‡
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(label2id),
    ignore_mismatched_sizes=True  # è‹¥é è¨“ç·´æ¨¡å‹è¼¸å‡ºç¶­åº¦ä¸åŒå‰‡å¼·åˆ¶å¿½ç•¥
)

# ======== 8. è¨“ç·´è¨­å®š ===========
training_args = TrainingArguments(
    output_dir="./bert_results",              # è¨“ç·´çµæœè¼¸å‡ºè³‡æ–™å¤¾
    num_train_epochs=3,                       # è¨“ç·´ epoch æ•¸
    per_device_train_batch_size=8,            # æ¯å€‹è£ç½®çš„è¨“ç·´ batch å¤§å°
    per_device_eval_batch_size=8,             # æ¯å€‹è£ç½®çš„é©—è­‰ batch å¤§å°
    learning_rate=2e-5,                       # å­¸ç¿’ç‡
    weight_decay=0.01,                        # æ¬Šé‡è¡°æ¸›
    logging_dir="./bert_logs",                # æ—¥èªŒè³‡æ–™å¤¾
    logging_steps=10,                         # æ¯ 10 æ­¥é©Ÿç´€éŒ„ä¸€æ¬¡
)

# ======== 9. è©•ä¼°æŒ‡æ¨™ ===========
accuracy = evaluate.load("accuracy")          # è¼‰å…¥ accuracy æŒ‡æ¨™
f1 = evaluate.load("f1")                      # è¼‰å…¥ F1-score æŒ‡æ¨™

# å®šç¾©è©•ä¼°å‡½å¼ï¼šå›å‚³ accuracy èˆ‡ macro-F1 åˆ†æ•¸
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="macro")["f1"]
    }

# ======== 10. Trainer è¨“ç·´ ===========
# ä½¿ç”¨ Hugging Face çš„ Trainer API é€²è¡Œè¨“ç·´
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print("ğŸš€ é–‹å§‹è¨“ç·´...")
trainer.train()  # é–‹å§‹è¨“ç·´æ¨¡å‹

# ======== 11. æ¨¡å‹å„²å­˜ ===========
# å„²å­˜è¨“ç·´å¥½çš„æ¨¡å‹å’Œ tokenizer
trainer.save_model("bert_models/roberta-news")
tokenizer.save_pretrained("bert_models/roberta-news")
