from datasets import load_dataset
from ckip_transformers.nlp import CkipWordSegmenter
import pandas as pd
import os

# ======== 1. è¼‰å…¥è³‡æ–™ ===========
print("ğŸ”„ è¼‰å…¥è³‡æ–™ä¸­...")
# å¾ Hugging Face è¼‰å…¥ zh-tw-articles-6k è³‡æ–™é›†çš„è¨“ç·´é›†
dataset = load_dataset("AWeirdDev/zh-tw-articles-6k")["train"]

# å°‡ title èˆ‡ content åˆä½µç‚ºä¸€æ®µæ–‡å­—ï¼Œä¸¦å–å¾—æ¨™ç±¤
texts = [item["title"] + "ã€‚" + item["content"] for item in dataset]
labels = dataset["tag"]  # å–å¾—å°æ‡‰çš„æ–°èåˆ†é¡æ¨™ç±¤

# ======== 2. ä½¿ç”¨ CKIP æ–·è© ===========
# åˆå§‹åŒ– CKIP æ–·è©å·¥å…·ï¼Œdevice=-1 è¡¨ç¤ºä½¿ç”¨ CPUï¼Œå¦‚æœ‰ GPU å¯æ”¹ç‚º device=0
ws_driver = CkipWordSegmenter(device=-1)

tokenized_texts = []
batch_size = 32  # æ¯æ¬¡è™•ç† 32 ç­†ï¼Œä»¥é¿å…è¨˜æ†¶é«”è² æ“”

# åˆ†æ‰¹æ–·è©è™•ç†
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    tokens = ws_driver(batch)  # æ–·è©
    tokenized_texts.extend([" ".join(tok) for tok in tokens])  # åˆä½µè©ç‚ºå­—ä¸²

# ======== 3. å„²å­˜è™•ç†çµæœç‚º CSV ===========
# å°‡æ–·è©çµæœèˆ‡æ¨™ç±¤è½‰ç‚º DataFrame
df = pd.DataFrame({"text": tokenized_texts, "label": labels})

# å»ºç«‹è³‡æ–™å¤¾ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
os.makedirs("data", exist_ok=True)

# å°‡çµæœå„²å­˜ç‚º CSV æª”æ¡ˆï¼Œä½¿ç”¨ UTF-8 with BOM ç·¨ç¢¼ä»¥æ”¯æ´ Excel
df.to_csv("data/tokenized_data.csv", index=False, encoding="utf-8-sig")
print("âœ… å·²å„²å­˜æ–·è©å¾Œè³‡æ–™åˆ° data/tokenized_data.csv")